{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot import dbm.gnu: No module named '_gdbm'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mdd424/CGinkgo/lib/python3.7/site-packages/pyprob/util.py:332: UserWarning: Empirical distributions on disk may perform slow because GNU DBM is not available. Please install and configure gdbm library for Python for better speed.\n",
      "  warnings.warn('Empirical distributions on disk may perform slow because GNU DBM is not available. Please install and configure gdbm library for Python for better speed.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import autograd\n",
    "\n",
    "from showerSim import invMass_ginkgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([407.9216, 230.9401, 230.9401, 230.9401], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jetM = 80. # parent mass -> W\n",
    "jetdir = th.tensor([1.,1.,1.], dtype=th.double) # direction\n",
    "jetP = 400. # magnitude\n",
    "jetvec = jetP * jetdir / th.norm(jetdir)\n",
    "\n",
    "jet4vec = th.cat((th.tensor([np.sqrt(jetP**2 + jetM**2)], dtype=th.double), jetvec))\n",
    "jet4vec.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition on the number of leaves\n",
    "def num_leaves_cut(self, jet):\n",
    "    return len(jet[\"leaves\"]) >= 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = invMass_ginkgo.SimulatorModel(#rate=[3., 1.5], # exponential dsitribution rate\n",
    "                                     jet_p=jet4vec,  # parent particle 4-vector\n",
    "                                     pt_cut=10.,  # minimum pT for resulting jet\n",
    "                                     Delta_0=th.tensor(jetM**2, requires_grad=True),  # parent particle mass squared -> needs tensor\n",
    "                                     M_hard=jetM,  # parent particle mass\n",
    "                                     minLeaves=30,  # minimum number of jet constituents\n",
    "                                     maxLeaves=40,  # maximum \" \"\n",
    "                                     bool_func=num_leaves_cut,\n",
    "                                     suppress_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Naming Scheme\n",
    "\n",
    "Direction of decay in CM frame:\n",
    "- \"phiCM\" + \"...\"\n",
    "- \"thetaCM_U\" + \"...\"\n",
    "\n",
    "Invariant Mass of decay products:\n",
    "- \"L_decay\" + \"...\"\n",
    "- \"R_decay\" + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_latent_variable(var):\n",
    "    latent_var_names = [\"phiCM\", \"thetaCM_U\", \"L_decay\", \"R_decay\"]\n",
    "    arr = np.array([var.name.startswith(x) for x in latent_var_names])\n",
    "    return arr.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-32.8037, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate = th.tensor([3, 1.5], dtype=th.double, requires_grad=True)\n",
    "\n",
    "trace = simulator.get_trace(inputs=rate)\n",
    "trace.log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.log_prob.retain_grad()\n",
    "trace.log_prob.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0081,  3.0347], dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a forward pass that also returns the joint score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = th.tensor([3, 1.5], dtype=th.double, requires_grad=True)\n",
    "\n",
    "data = simulator.augmented_data(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if the expectation of the joint score is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0        \n",
      "Iteration: 1000     \n",
      "Iteration: 2000     \n",
      "Iteration: 3000     \n",
      "Iteration: 4000     \n",
      "Iteration: 5000     \n",
      "Iteration: 6000     \n",
      "Iteration: 7000     \n",
      "Iteration: 8000     \n",
      "Iteration: 9000     \n",
      "[0.3379, 3.3108]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.33807291, 3.31132955])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate = th.tensor([3, 1.5], dtype=th.double, requires_grad=True)\n",
    "\n",
    "running_score = np.zeros(2)\n",
    "for i in range(10000):\n",
    "    data = simulator.augmented_data(rate)\n",
    "    running_score += data[\"joint_score\"].detach().numpy()\n",
    "    rate.grad.zero_()\n",
    "    if ((i+1) % 1000) == 0:\n",
    "        print(\" \"*20, end=\"\\r\")\n",
    "        print(\"Iteration:\", i)\n",
    "    if ((i+1) % 50) == 0:\n",
    "        print(\"[{:.4f}, {:.4f}]\".format(running_score[0]/(i+1), running_score[1]/(i+1)), end=\"\\r\")\n",
    "running_score / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE CODE BELOW IS IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dist = th.distributions.Uniform(0.1, 10)\n",
    "\n",
    "num_thetas = 5 #100\n",
    "num_samples_\n",
    "\n",
    "theta1 = th.tensor([3, 1.5], dtype=th.double, requires_grad=True)  # Reference parameter\n",
    "num_thetas = 5 #100\n",
    "num_samples_per_theta = 200 #5000\n",
    "theta2 = theta_dist.sample((num_thetas, 2))\n",
    "\n",
    "for i in range(num_thetas):\n",
    "    theta2_1 = theta_dist.sample()\n",
    "    theta2_2 = theta_dist.sample()\n",
    "    theta2 = th.tensor([theta2_1, theta2_2], dtype=th.double, requires_grad=True)\n",
    "    \n",
    "    for j in range(num_samples_per_theta):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NN to learn the likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, width, dropout=0.2):\n",
    "        super(DeepRegressor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.width = width\n",
    "        self.p_drop = dropout\n",
    "        # Layers\n",
    "        #  - Linear\n",
    "        #  - Activation\n",
    "        #  - Batch Normalization\n",
    "        #  - Dropout\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.activation_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        # Add the first hidden layer\n",
    "        self.linear_layers.append(nn.Linear(input_dim, input_dim*width))\n",
    "        self.activation_layers.append(nn.PReLU())\n",
    "        self.norm_layers.append(nn.BatchNorm1d(input_dim*width))\n",
    "        # Add the rest\n",
    "        for i in range(num_layers-1):\n",
    "            self.linear_layers.append(nn.Linear(input_dim*width, input_dim*width))\n",
    "            self.activation_layers.append(nn.PReLU())\n",
    "            self.norm_layers.append(nn.BatchNorm1d(input_dim*width))\n",
    "        # Initialize the weights for the linear layers in a special way\n",
    "        self.init_kaiming(self.linear_layers)\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(input_dim*width, 1)\n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dropout(self.norm_layers[i](self.activation_layers[i](self.linear_layers[i](x))))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def init_kaiming(moduleList):\n",
    "        for linear_layer in moduleList:\n",
    "            nn.init.kaiming_normal_(linear_layer.weight, a=0.25)\n",
    "            nn.init.constant_(linear_layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8556)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.distributions.Uniform(0.1, 10).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CGinkgo] *",
   "language": "python",
   "name": "conda-env-CGinkgo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
